RLHF Pipeline - Step 1 Workflow
================================================================================

                        RLHF SYSTEM ARCHITECTURE
                                                                               
┌─────────────────────────────────────────────────────────────────────────┐
│                         PHASE 1: DATA COLLECTION                         │
│                                                                           │
│  User Query → AI Response → SME Reviews → Ratings (1-5)                 │
│                                                                           │
│  Database: rlhf_interactions                                             │
│  ┌──────────────┬──────────────┬──────────┬──────────────────┐         │
│  │ user_prompt  │ ai_response  │ rating   │ feedback_comment │         │
│  ├──────────────┼──────────────┼──────────┼──────────────────┤         │
│  │ "Symptoms    │ "Increased   │    5     │ "Accurate..."    │         │
│  │  of diabetes"│  thirst..."  │          │                  │         │
│  └──────────────┴──────────────┴──────────┴──────────────────┘         │
│                                                                           │
│  Goal: Collect 50+ rated interactions                                    │
└─────────────────────────────────────────────────────────────────────────┘
                                   │
                                   ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                        PHASE 2: MODEL TRAINING                           │
│                                                                           │
│  Command: python train_reward_sbert.py                                   │
│                                                                           │
│  ┌────────────────────────────────────────────────────────────┐         │
│  │ 1. Load Feedback                                            │         │
│  │    • Read rlhf_interactions (rating IS NOT NULL)           │         │
│  │    • Filter: 50+ samples required                          │         │
│  └────────────────────────────────────────────────────────────┘         │
│                         │                                                 │
│                         ▼                                                 │
│  ┌────────────────────────────────────────────────────────────┐         │
│  │ 2. Prepare Dataset                                          │         │
│  │    • Combine: prompt + " </s> " + response                 │         │
│  │    • Label: 1 if rating >= 4, else 0                       │         │
│  └────────────────────────────────────────────────────────────┘         │
│                         │                                                 │
│                         ▼                                                 │
│  ┌────────────────────────────────────────────────────────────┐         │
│  │ 3. Generate Embeddings (SBERT)                             │         │
│  │    • Model: all-MiniLM-L6-v2                               │         │
│  │    • Output: 384-dimensional vectors                       │         │
│  └────────────────────────────────────────────────────────────┘         │
│                         │                                                 │
│                         ▼                                                 │
│  ┌────────────────────────────────────────────────────────────┐         │
│  │ 4. Train Classifier                                         │         │
│  │    • Algorithm: Logistic Regression                        │         │
│  │    • Train/Test Split: 88% / 12%                           │         │
│  │    • Class Weight: Balanced                                │         │
│  └────────────────────────────────────────────────────────────┘         │
│                         │                                                 │
│                         ▼                                                 │
│  ┌────────────────────────────────────────────────────────────┐         │
│  │ 5. Evaluate & Save                                          │         │
│  │    • Metrics: Accuracy, AUC-ROC                            │         │
│  │    • Save: reward_model.joblib                             │         │
│  │    • Log: rlhf_reward_model_training table                 │         │
│  └────────────────────────────────────────────────────────────┘         │
│                                                                           │
│  Output: reward_model.joblib (trained classifier)                        │
└─────────────────────────────────────────────────────────────────────────┘
                                   │
                                   ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                        PHASE 3: INFERENCE                                │
│                                                                           │
│  Integration: Flask /data or /data-html endpoint                         │
│                                                                           │
│  ┌────────────────────────────────────────────────────────────┐         │
│  │ User Query Received                                         │         │
│  │   "What are the symptoms of diabetes?"                     │         │
│  └────────────────────────────────────────────────────────────┘         │
│                         │                                                 │
│                         ▼                                                 │
│  ┌────────────────────────────────────────────────────────────┐         │
│  │ RAG System Generates Candidates                            │         │
│  │   1. Answer from Vector DB (score: unknown)                │         │
│  │   2. Answer from Wikipedia (score: unknown)                │         │
│  │   3. Answer from ArXiv (score: unknown)                    │         │
│  └────────────────────────────────────────────────────────────┘         │
│                         │                                                 │
│                         ▼                                                 │
│  ┌────────────────────────────────────────────────────────────┐         │
│  │ RLHF Re-ranking (rlhf_reranker.py)                         │         │
│  │                                                             │         │
│  │   from rlhf_reranker import rerank_candidates              │         │
│  │                                                             │         │
│  │   ranked = rerank_candidates(query, candidates)            │         │
│  │                                                             │         │
│  │   For each candidate:                                       │         │
│  │     1. Create text: query + " </s> " + answer              │         │
│  │     2. Generate SBERT embedding                            │         │
│  │     3. Score with reward model → [0, 1]                    │         │
│  │     4. Sort by score (highest first)                       │         │
│  └────────────────────────────────────────────────────────────┘         │
│                         │                                                 │
│                         ▼                                                 │
│  ┌────────────────────────────────────────────────────────────┐         │
│  │ Re-ranked Candidates                                        │         │
│  │   1. Answer from ArXiv (score: 0.87) ← BEST                │         │
│  │   2. Answer from Vector DB (score: 0.65)                   │         │
│  │   3. Answer from Wikipedia (score: 0.42)                   │         │
│  └────────────────────────────────────────────────────────────┘         │
│                         │                                                 │
│                         ▼                                                 │
│  ┌────────────────────────────────────────────────────────────┐         │
│  │ Return Best Answer                                          │         │
│  │   best_answer = ranked[0]['text']                          │         │
│  │   Send to user ✓                                           │         │
│  └────────────────────────────────────────────────────────────┘         │
└─────────────────────────────────────────────────────────────────────────┘
                                   │
                                   ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                    PHASE 4: CONTINUOUS IMPROVEMENT                       │
│                                                                           │
│  ┌────────────────────────────────────────────────────────────┐         │
│  │ Collect More Feedback                                       │         │
│  │   • Users rate AI responses                                │         │
│  │   • SMEs review and correct                                │         │
│  │   • Store in rlhf_interactions                             │         │
│  └────────────────────────────────────────────────────────────┘         │
│                         │                                                 │
│                         ▼                                                 │
│  ┌────────────────────────────────────────────────────────────┐         │
│  │ Retrain Periodically                                        │         │
│  │   • Weekly/Monthly: python train_reward_sbert.py           │         │
│  │   • Model improves with more data                          │         │
│  │   • Track accuracy over time                               │         │
│  └────────────────────────────────────────────────────────────┘         │
│                         │                                                 │
│                         ▼                                                 │
│  ┌────────────────────────────────────────────────────────────┐         │
│  │ Monitor Performance                                         │         │
│  │   • Accuracy metrics (target: 80%+)                        │         │
│  │   • User satisfaction scores                               │         │
│  │   • Response quality trends                                │         │
│  └────────────────────────────────────────────────────────────┘         │
└─────────────────────────────────────────────────────────────────────────┘

================================================================================
                           KEY COMPONENTS
================================================================================

Files Created:
├── model_utils.py              - Database & persistence utilities
├── train_reward_sbert.py       - Training pipeline
├── rlhf_reranker.py            - Inference & re-ranking
├── test_rlhf_pipeline.py       - Test suite
├── RLHF_STEP1_README.md        - Full documentation
├── RLHF_QUICKSTART.md          - Quick start guide
└── RLHF_IMPLEMENTATION_SUMMARY.md - This summary

Database Tables:
├── rlhf_interactions           - User prompts, responses, ratings
└── rlhf_reward_model_training  - Training run logs

Model Files:
└── reward_model.joblib         - Trained classifier (created after training)

================================================================================
                           QUICK COMMANDS
================================================================================

Install Dependencies:
  pip install sentence-transformers sqlalchemy joblib

Check Setup:
  python -c "import rlhf_reranker; print('✅ Ready!')"

Check Training Data:
  python -c "from model_utils import engine; from sqlalchemy import text; \
  with engine.connect() as c: \
      r = c.execute(text('SELECT COUNT(*) FROM rlhf_interactions WHERE rating IS NOT NULL')); \
      print(f'Rated samples: {r.scalar()}')"

Train Model (need 50+ ratings):
  python train_reward_sbert.py

Test Model:
  python rlhf_reranker.py

Run Full Tests:
  python test_rlhf_pipeline.py

================================================================================
                          PERFORMANCE METRICS
================================================================================

Training:
• Time: ~30 seconds (200 samples)
• CPU: Low (single-threaded)
• Memory: ~200 MB peak

Inference:
• Latency: ~10ms per candidate
• Throughput: ~100 candidates/second
• Memory: ~50 MB

Model:
• Size: ~50 KB (very lightweight)
• Accuracy: 70-85% (depends on data)

================================================================================
                          SUCCESS PATH
================================================================================

Week 1: Setup & Collection
  ✓ Install dependencies
  ✓ Verify setup
  → Collect 50+ SME ratings

Week 2: First Training
  → Train initial model
  → Achieve 70-75% accuracy
  → Integrate into Flask

Week 3-4: Monitoring
  → Monitor performance
  → Collect feedback
  → Reach 100+ ratings

Month 2: Optimization
  → Retrain with 200+ ratings
  → Achieve 80-85% accuracy
  → Consider Step 2 (Transformer)

================================================================================
